{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic PyTorch Implementation of EfficientNetB3 using classification instead of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "import torchmetrics\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Model\n",
    "MODEL_TITLE = \"test_mk0\"\n",
    "\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b3\", in_channels = 3, num_classes = 1000)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Will Be Used\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"CPU Will Be Used\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 30)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading TODO since we'll have to re-organize some of the formatting\n",
    "class Dataset:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset = np.array(dataset_name)\n",
    "dataset_train = Dataset(self.cfg, dataset_name=\"train_set\", transform=self.ttfms)\n",
    "train_loader = DataLoader(dataset_train, \n",
    "                            batch_size=self.cfg['TRAIN']['BATCH_SIZE'], \n",
    "                            num_workers=self.cfg['DATASET']['NUM_WORKERS'])\n",
    "\n",
    "dataset_val = ClassifierDataset(self.cfg, dataset_name=\"val_set\", transform=self.vtfms)\n",
    "val_loader = DataLoader(dataset_val, \n",
    "                        batch_size=self.cfg['TEST']['BATCH_SIZE'], \n",
    "                        num_workers=self.cfg['DATASET']['NUM_WORKERS'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Functions\n",
    "def train_fn(train_loader):\n",
    "    model.train()\n",
    "    loop = tqdm.tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "    for batch_idx, data in enumerate(loop):\n",
    "        imgs = data['image'].to(device)\n",
    "        labels = data['class'].squeeze(1).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = loss(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=total_loss/(batch_idx+1))\n",
    "    \n",
    "    return total_loss/len(loop)\n",
    "\n",
    "def val_fn(val_loader):\n",
    "    model.eval()\n",
    "    loop = tqdm.tqdm(val_loader)\n",
    "    total_loss = 0\n",
    "    \n",
    "    pr, re, ac, f1 = 0, 0 ,0 ,0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(loop):\n",
    "            imgs = data['image'].to(device)\n",
    "            labels = data['class'].squeeze(1).to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(imgs)\n",
    "            loss = loss(outputs, labels)\n",
    "                \n",
    "            # calc ap and ar metrics\n",
    "            output_on_cpu = outputs.detach().cpu().softmax(dim=-1)\n",
    "            labels_on_cpu = labels.detach().cpu()\n",
    "            pr += torchmetrics.functional.precision(output_on_cpu, labels_on_cpu)\n",
    "            re += torchmetrics.functional.recall(output_on_cpu, labels_on_cpu)\n",
    "            ac += torchmetrics.functional.accuracy(output_on_cpu.argmax(axis=-1), labels_on_cpu)\n",
    "            f1 += torchmetrics.functional.f1(output_on_cpu, labels_on_cpu)\n",
    "\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # update tqdm loop\n",
    "            loop.set_postfix(loss=total_loss/(batch_idx+1))        \n",
    "\n",
    "    return pr/len(loop), re/len(loop), ac/len(loop), f1/len(loop), total_loss/len(loop)\n",
    "\n",
    "def save_checkpoint(model, f1, loss, model_name):\n",
    "    print(f\"=> Saving checkpoint with validation f1-score: {f1}, and validation loss: {loss}\")\n",
    "    torch.save(model, model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "history ={'loss': [], 'val_loss': [], 'val_f1': [], 'val_pr': [], 'val_acc': [], 'val_re':[]}\n",
    "comp = np.inf\n",
    "max_epoch = 1000\n",
    "for epoch in range(max_epoch):\n",
    "    loss = train_fn(train_loader)\n",
    "    pr, re, ac, f1, val_loss = val_fn(val_loader)\n",
    "    \n",
    "    history['loss'].append(loss)\n",
    "    history['val_loss'].append(val_loss) \n",
    "    history['val_pr'].append(pr) \n",
    "    history['val_re'].append(re)\n",
    "    history['val_acc'].append(ac) \n",
    "    history['val_f1'].append(f1) \n",
    "\n",
    "    if val_loss < comp:\n",
    "        comp = val_loss\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"history\": history\n",
    "        }\n",
    "        save_checkpoint(checkpoint, f1, val_loss, os.path.join(os.getcwd(),'model_output.pt'))\n",
    "    else:\n",
    "        print(f\"=> The validation loss did not improve from: {comp}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
